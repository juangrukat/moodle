```jsonc
{
  //============================================================
  // Moodle GIFT Style Guide (v2.0 – July 2025)
  // Purpose
  // --------
  // 1. Provide an LLM with rock‑solid rules & templates for
  //    writing Moodle‑compliant GIFT questions.
  // 2. Supply a **feature‑tag vocabulary** that lets the LLM
  //    map what it sees in a source passage → the most
  //    appropriate question TYPE to generate.
  //
  // In practice, the workflow is:
  //   ◦ LLM analyses the passage and assigns one or more
  //     *text_feature_tags* to what it finds.
  //   ◦ For each feature, look up the question types whose
  //     `feature_tags` overlap.
  //   ◦ Pick the best‑fit templates and fill in PARAMETERS to
  //     create the desired number of questions (e.g., 20).
  //============================================================

  "meta": {
    "encoding": "UTF‑8",                      // Always save files in UTF‑8
    "blank_line_between_questions": true,     // One blank line AFTER every complete question. No blank lines inside the answer block; a single newline just before the closing brace `}` is fine.
    "title_required": true,                   // Every question needs ::Title::
    "teacher_comment_required": true,         // Every ~wrong answer AND every wrong branch of T/F needs #feedback. For clarity, we strongly recommend feedback for correct answers too.
    "escape_rule": "Prefix ~ = # { } : with a backslash (\) when they appear as literals. Inside feedback text, any literal # must also be escaped (use \#) so Moodle doesn't treat it as a new feedback marker."
  },

  //------------------------------------------------------------
  // Text‑Feature Tag Taxonomy
  // -------------------------
  // Exactly 20 tags that describe salient structures or facts a
  // passage might contain.  They are attached to *question
  // types* (see below) — NOT to individual question instances.
  //------------------------------------------------------------
  "text_feature_tags": [
    // literal / factual
    "numeric_value",      // explicit numbers, units, quantitative data
    "binary_statement",   // claims that are either true or false
    "named_entity",       // proper names: people, places, titles, terms
    "chronology",         // dates, timelines, ordered historical events
    "list_items",         // bullet lists or comma‑separated enumerations
    "paired_items",       // A→B correspondences (term : definition)
    "stepwise_process",   // ordered procedure or algorithm
    "classification",     // category / sub‑category relationships
    "comparison",         // explicit similarities / differences
    "cause_effect",       // stated causal relationships
    // higher‑order / contextual
    "concept_definition", // sentence that defines or explains a term
    "example_sentence",   // sentence gives example of concept in action
    "quote",              // quotations that can be completed or attributed
    "formula_or_equation",// math/chem expressions present in text
    "graph_or_table",     // visual data referenced by the prose
    "conditional_statement",// if … then … constructions
    "interpretation_required",// inference beyond literal text
    "process_output",     // mapping of input → output/result
    "ranking_order",      // best → worst, largest → smallest, etc.
    "exception_rule"      // rule plus its exceptions stated
  ],

  //------------------------------------------------------------
  // Question Type Templates
  // -----------------------
  // Each template has:
  //   • description   – plain‑language summary.
  //   • feature_tags  – which text features signal this type is
  //                     a strong candidate.
  //   • template      – skeletal GIFT with ${placeholders}.
  //   • parameters    – required inputs the LLM must provide.
  //------------------------------------------------------------
  "question_types": {

    "multiple_choice_single": {
      "description": "Exactly one correct answer (radio‑button).",
      "feature_tags": [
        "named_entity", "numeric_value", "concept_definition",
        "list_items", "example_sentence", "cause_effect"
      ],
      "template": "::${title}:: ${question_text} {\n =${correct_answer} #${correct_feedback}\n ~${wrong_answer_1} #${feedback_1}\n ~${wrong_answer_2} #${feedback_2}\n ...\n}",
      "parameters": {
        "title": "3‑12 word summary of the ask.",
        "question_text": "Prompt. May start with [html]|[markdown]|[plain]|[moodle].",
        "answers[]": {
          "text": "Answer text",
          "is_correct": "true | false (only one true)",
          "weight": "Optional %n% for partial credit (rare)",
          "feedback": "Teacher explanation (required for wrong answers). **Do not include an unescaped # inside**; use \# if # is needed inside the feedback text."
        },
        "shuffle": "true | false – let Moodle randomise order",
        "default_grade": "Numeric points for full credit"
      }
    },

    "multiple_choice_multiple": {
      "description": "Checkboxes; several answers may be partially correct.",
      "feature_tags": [
        "list_items", "comparison", "classification",
        "stepwise_process", "ranking_order", "exception_rule"
      ],
      "template": "::${title}:: ${question_text} {\n ~%${neg_weight}%${distractor} #${feedback_d}\n ~%${pos_weight}%${correct_part} #${feedback_c}\n ...\n}",
      "parameters": {
        "answers[]": {
          "text": "Answer text",
          "weight": "Positive or negative % weight",
          "feedback": "Teacher explanation. Escape internal # as \#"
        },
        "total_positive_weight": "Sum of all positive weights ≤100",
        "min_negative_weight": "Use at least ‑50% to deter select‑all"
      }
    },

    "true_false": {
      "description": "Binary proposition (T/F).",
      "feature_tags": [ "binary_statement", "cause_effect", "conditional_statement" ],
      "template": "::${title}:: ${statement} {${T_or_F}#${feedback_wrong}#${feedback_right}}",
      "notes": "Keep feedback on the same line as the T/F letter; optionally insert one newline before the final `}`.",
      "parameters": {
        "statement": "Text of the claim.",
        "correct_value": "T | F (TRUE | FALSE)",
        "feedback_wrong": "Required. Shown when learner selects the wrong value. Must not include unescaped #.",
        "feedback_right": "Optional but encouraged. Shown when learner selects the correct value. Must not include unescaped #."
      }
    }
    },

    "short_answer": {
      "description": "Learner types a short text answer; synonyms allowed.",
      "feature_tags": [ "named_entity", "concept_definition", "quote" ],
      "template": "::${title}:: ${question_text} {\n =${answer1} #${fb1}\n =${answer2} #${fb2}\n ...\n}",
      "parameters": {
        "answers[]": {
          "text": "Accepted variant",
          "weight": "Optional %n% for partial credit",
          "feedback": "Explanation for this variant"
        },
        "usecase": "0 (case‑insensitive) | 1 (case‑sensitive)"
      }
    },

    "matching": {
      "description": "Learner pairs items from two columns.",
      "feature_tags": [ "paired_items", "classification", "process_output" ],
      "template": "::${title}:: ${question_text} {\n =${left_1} -> ${right_1}\n =${left_2} -> ${right_2}\n =${left_3} -> ${right_3}\n}",
      "parameters": {
        "pairs[]": { "left": "String", "right": "String" },
        "min_pairs": 3
      }
    },

    "missing_word": {
      "description": "Embedded answers inside a sentence (gap‑fill).",
      "feature_tags": [ "quote", "concept_definition", "numeric_value", "named_entity" ],
      "template": "::${title}:: ${sentence_with_gap}",
      "example": "Don Quixote and Sancho Panza started their {~work #Not work. ~job #Wrong. =adventure #Correct!}.",
      "parameters": {
        "sentence_with_gap": "Sentence that already contains the brace‑enclosed answer set.",
        "answers_per_gap": "At least one =correct (with feedback) and ≥1 ~wrong (each WITH feedback).",
        "feedback": "Every option—wrong OR correct—must carry exactly one #feedback segment. Escape internal # as \#."
      }
    },

    "numerical": {
      "description": "Learner enters a number within tolerance or range.",
      "feature_tags": [ "numeric_value", "formula_or_equation", "graph_or_table" ],
      "template": "::${title}:: ${question_text} {#${answer}:${tolerance}}",
      "parameters": {
        "answer": "Central numeric value OR min..max span",
        "tolerance": "Accepted ± range (e.g., 0.5)",
        "partial_credit_ranges": "Optional =%n%value:tolerance bands",
        "feedback_each_range": "#comment after each range (escape internal # as \#)"
      }
    },

    "essay": {
      "description": "Open response to be graded manually.",
      "feature_tags": [ "interpretation_required", "cause_effect", "comparison", "analysis" ],
      "template": "::${title}:: ${prompt} {}",
      "parameters": {
        "prompt": "Question or writing task.",
        "grader_info": "Directions for human grader (optional)",
        "expected_word_count": "Suggested range (optional)"
      }
    },

    "description": {
      "description": "Non‑interactive informational text block.",
      "feature_tags": [ "graph_or_table", "quote" ],
      "template": "::${title}:: ${content}",
      "parameters": {
        "content": "Plaintext or formatted information (no answer block)"
      }
    }
  }
}
```
