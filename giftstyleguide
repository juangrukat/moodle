```jsonc
{
  //============================================================
  // Moodle GIFT Style Guide (v2.0 – July 2025)
  // Purpose
  // --------
  // 1. Provide an LLM with rock‑solid rules & templates for
  //    writing Moodle‑compliant GIFT questions.
  // 2. Supply a **feature‑tag vocabulary** that lets the LLM
  //    map what it sees in a source passage → the most
  //    appropriate question TYPE to generate.
  //
  // In practice, the workflow is:
  //   ◦ LLM analyses the passage and assigns one or more
  //     *text_feature_tags* to what it finds.
  //   ◦ For each feature, look up the question types whose
  //     `feature_tags` overlap.
  //   ◦ Pick the best‑fit templates and fill in PARAMETERS to
  //     create the desired number of questions (e.g., 20).
  //============================================================

  "meta": {
    "encoding": "UTF‑8",                      // Always save files in UTF‑8
    "blank_line_between_questions": true,     // Insert exactly one blank line AFTER each question's closing }. Absolutely **no empty lines** are allowed inside an answer block.
    "title_required": true,                   // Every question needs ::Title::
    "teacher_comment_required": true,         // Every ~wrong answer AND every wrong branch of T/F needs #feedback. For clarity, we strongly recommend feedback for correct answers too.
    "escape_rule": "Prefix ~ = # { } : with a backslash (\) when they appear as literals. Inside feedback text, you may use \# to show a literal hash; **there must be only one unescaped # per answer line**, which divides the answer from its feedback."
  },

  //------------------------------------------------------------
  // Text‑Feature Tag Taxonomy
  // -------------------------
  // Exactly 20 tags that describe salient structures or facts a
  // passage might contain.  They are attached to *question
  // types* (see below) — NOT to individual question instances.
  //------------------------------------------------------------
  "text_feature_tags": [
    // literal / factual
    "numeric_value",      // explicit numbers, units, quantitative data
    "binary_statement",   // claims that are either true or false
    "named_entity",       // proper names: people, places, titles, terms
    "chronology",         // dates, timelines, ordered historical events
    "list_items",         // bullet lists or comma‑separated enumerations
    "paired_items",       // A→B correspondences (term : definition)
    "stepwise_process",   // ordered procedure or algorithm
    "classification",     // category / sub‑category relationships
    "comparison",         // explicit similarities / differences
    "cause_effect",       // stated causal relationships
    // higher‑order / contextual
    "concept_definition", // sentence that defines or explains a term
    "example_sentence",   // sentence gives example of concept in action
    "quote",              // quotations that can be completed or attributed
    "formula_or_equation",// math/chem expressions present in text
    "graph_or_table",     // visual data referenced by the prose
    "conditional_statement",// if … then … constructions
    "interpretation_required",// inference beyond literal text
    "process_output",     // mapping of input → output/result
    "ranking_order",      // best → worst, largest → smallest, etc.
    "exception_rule"      // rule plus its exceptions stated
  ],

  //------------------------------------------------------------
  // Question Type Templates
  // -----------------------
  // Each template has:
  //   • description   – plain‑language summary.
  //   • feature_tags  – which text features signal this type is
  //                     a strong candidate.
  //   • template      – skeletal GIFT with ${placeholders}.
  //   • parameters    – required inputs the LLM must provide.
  //------------------------------------------------------------
  "question_types": {

    "multiple_choice_single": {
      "description": "Exactly one correct answer (radio‑button).",
      "feature_tags": [
        "named_entity", "numeric_value", "concept_definition",
        "list_items", "example_sentence", "cause_effect"
      ],
      "template": "::${title}:: ${question_text} {
=${correct_answer} #${correct_feedback}
~${wrong_answer_1} #${feedback_1}
~${wrong_answer_2} #${feedback_2}
...
}",
      "format_rules": [
        $1,
        "Each answer line must contain **exactly one** #feedback delimiter. If a # is needed inside feedback text, escape it as \#."
      ],
      "parameters": {
        "title": "3‑12 word summary of the ask.",
        "question_text": "Prompt. May start with [html]|[markdown]|[plain]|[moodle].",
        "answers[]": {
          "text": "Answer text (single line).",
          "is_correct": "true | false (only one true)",
          "weight": "Optional %n% for partial credit (rare)",
          "feedback": "Teacher explanation (required for wrong answers). Do NOT include a line‑break or unescaped # inside; use \# if # is needed."
        },
        "shuffle": "true | false – let Moodle randomise order",
        "default_grade": "Numeric points for full credit"
      }
    },

    "multiple_choice_multiple": {
      "description": "Checkboxes; several answers may be partially correct.",
      "feature_tags": [
        "list_items", "comparison", "classification",
        "stepwise_process", "ranking_order", "exception_rule"
      ],
      "template": "::${title}:: ${question_text} {
~%${weight_1}%${answer_1} #${feedback_1}
~%${weight_2}%${answer_2} #${feedback_2}
...
}",
      "format_rules": [
        "Begin each answer line with a single `~` (never `=`) followed **immediately** by the optional `%weight%` block.",
        "Do **not** place a second `~` or `=` after the `%weight%`; the answer text starts right away, e.g., `~%50%This is the text`.",
        "Weights may be positive or negative, but the sum of positive weights should not exceed 100%.",
        "Each answer line must contain **exactly one** `#` feedback delimiter. Escape literal # inside feedback as `\#`.",
        "Keep each answer+feedback on one physical line; no blank lines inside the block.",
        "Terminate the block with a single `}` on its own line (no blank line before it)."
      ],
      "parameters": {
        "answers[]": {
          "text": "Answer text (single line, must not start with ~ or =).",
          "weight": "Positive or negative integer between ‑100 and 100 (percent).",
          "feedback": "Teacher explanation. Escape internal # as \#"
        },
        "total_positive_weight": "Sum of all positive weights ≤100",
        "min_negative_weight": "Use at least ‑50% to deter select‑all"
      }
    },
      "parameters": {
        "answers[]": {
          "text": "Answer text (single line).",
          "weight": "Positive or negative % weight",
          "feedback": "Teacher explanation. Escape internal # as \#"
        },
        "total_positive_weight": "Sum of all positive weights ≤100",
        "min_negative_weight": "Use at least ‑50% to deter select‑all"
      }
    }

    "true_false": {
      "description": "Binary proposition (T/F). Exactly ONE letter (the correct value) appears inside the braces.",
      "feature_tags": [ "binary_statement", "cause_effect", "conditional_statement" ],
      "template": "::${title}:: ${statement} {${T_or_F}#${wrong_fb}#${right_fb}}",
      "format_rules": [
        "Only the correct letter (T or F) is allowed — no extra ~ lines.",
        "After the letter: first # segment = feedback shown when learner is **wrong** (chooses the other letter).",
        "Second # segment (optional) = feedback shown when learner is **right**.",
        "Content of each feedback segment must match its purpose and must not contradict the scoring logic.",
        "Keep letter and both feedback segments on one physical line.",
        "Close with } on the very next line, no blank spacer above it."
      ],
      "parameters": {
        "wrong_fb": "Required. Feedback for the incorrect choice.",
        "right_fb": "Optional. Feedback for the correct choice."
      }
    },
      "parameters": {
        "statement": "Text of the claim.",
        "correct_value": "T | F (TRUE | FALSE)",
        "feedback_wrong": "Required. Shown when learner selects the wrong value. Must not contain unescaped # or line‑breaks.",
        "feedback_right": "Optional but encouraged. Shown when learner selects the correct value. Must not contain unescaped # or line‑breaks."
      }
    },
      "parameters": {
        "statement": "Text of the claim.",
        "correct_value": "T | F (TRUE | FALSE)",
        "feedback_wrong": "Required. Shown when learner selects the wrong value. Must not contain unescaped # or line‑breaks.",
        "feedback_right": "Optional but encouraged. Shown when learner selects the correct value. Must not contain unescaped # or line‑breaks."
      }
    },

    "short_answer": {
      "description": "Learner types a short text answer; synonyms allowed.",
      "feature_tags": [ "named_entity", "concept_definition", "quote" ],
      "template": "::${title}:: ${question_text} {\n =${answer1} #${fb1}\n =${answer2} #${fb2}\n ...\n}",
      "parameters": {
        "answers[]": {
          "text": "Accepted variant",
          "weight": "Optional %n% for partial credit",
          "feedback": "Explanation for this variant"
        },
        "usecase": "0 (case‑insensitive) | 1 (case‑sensitive)"
      }
    },

    "matching": {
      "description": "Learner pairs items from two columns.",
      "feature_tags": [ "paired_items", "classification", "process_output" ],
      "template": "::${title}:: ${question_text} {
=${left_1} -> ${right_1}
=${left_2} -> ${right_2}
=${left_3} -> ${right_3}
}",
      "format_rules": [
        $1,
        "Each answer line must contain **exactly one** #feedback delimiter. If a # is needed inside feedback text, escape it as \#."
      ],
      "parameters": {
        "pairs[]": { "left": "String", "right": "String" },
        "min_pairs": 3
      }
    }pairs[]": { "left": "String", "right": "String" },
        "min_pairs": 3
      }
    },

    "missing_word": {
      "description": "Embedded answers inside a sentence (gap‑fill).",
      "feature_tags": [ "quote", "concept_definition", "numeric_value", "named_entity" ],
      "template": "::${title}:: ${sentence_with_gap}",
      "example": "Don Quixote and Sancho Panza started their {~work #Not work. ~job #Wrong. =adventure #Correct!}.",
      "parameters": {
        "sentence_with_gap": "Sentence that already contains the brace‑enclosed answer set.",
        "answers_per_gap": "At least one =correct (with feedback) and ≥1 ~wrong (each WITH feedback).",
        "feedback": "Every option—wrong OR correct—must carry exactly one #feedback segment. Escape internal # as \#."
      }
    },

    "numerical": {
      "description": "Numeric response with tolerance OR range (mutually exclusive).",
      "feature_tags": [ "numeric_value", "formula_or_equation", "graph_or_table" ],
      "template_value_tol": "::${title}:: ${question_text} {#${value}:${tolerance}}",
      "template_range": "::${title}:: ${question_text} {#${min}..${max}}",
      "template_value_tol_fb": "::${title}:: ${question_text} {
=${value}:${tolerance} #${feedback}
}",,
      "format_rules": [
        "Choose **exactly one** style: (a) central value + :tolerance **or** (b) min..max span — never both together.",
        "If you include feedback, switch to the multi‑line variant and prefix each answer with `=` (no leading #).",
        "No blank lines inside the block; close with } immediately after the last answer line."
      ],
      "parameters": {
        "value": "Central numeric value (for tolerance form)",
        "tolerance": "± range allowed",
        "min": "Lower bound (for span form)",
        "max": "Upper bound (for span form)",
        "feedback": "Optional feedback for multi‑line variant (escape internal # as \#)"
      }
    },
      "parameters": {
        "answer": "Central numeric value OR min..max span",
        "tolerance": "Accepted ± range (e.g., 0.5)",
        "feedback": "Optional feedback when using the multi‑line variant (escape internal # as \#)"
      }
    }

    "essay": {
      "description": "Open response to be graded manually (no automatic grading).",
      "feature_tags": [ "interpretation_required", "cause_effect", "comparison", "analysis" ],
      "template": "::${title}:: ${prompt} {}",
      "format_rules": [
        "The curly‑brace pair `{}` must appear on the SAME line as the prompt (no line breaks between ? and {}).",
        "Place the closing `}` at the very end of that line followed by **one newline**.",
        "Insert exactly one blank line AFTER `}` to delimit the next question (per meta rule)."
      ],
      "parameters": {
        "prompt": "Question or writing task.",
        "grader_info": "Directions for human grader (optional)",
        "expected_word_count": "Suggested range (optional)"
      }
    }

    "description": {
      "description": "Non‑interactive informational text block.",
      "feature_tags": [ "graph_or_table", "quote" ],
      "template": "::${title}:: ${content}",
      "parameters": {
        "content": "Plaintext or formatted information (no answer block)"
      }
    }
  }
}
```
